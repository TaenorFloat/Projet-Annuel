{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GénérateurDeBiographies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U7BgcRyuU2lkbPJaQwX58UJY9NqnFm9w",
      "authorship_tag": "ABX9TyNJyFFSw9lhvn6ua6nsH144",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaenorFloat/Projet-Annuel/blob/master/G%C3%A9n%C3%A9rateurDeBiographies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PASvxrryJUAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "253799a0-d554-4161-d202-2c21883f6fa4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UFBZTgYJvci",
        "colab_type": "code",
        "outputId": "18ba7982-72a1-4c63-935c-c09974fa1fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "! pip install sparqlwrapper\n",
        "! python -m spacy download fr\n",
        "! python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sparqlwrapper in /usr/local/lib/python3.6/dist-packages (1.8.5)\n",
            "Requirement already satisfied: rdflib>=4.0 in /usr/local/lib/python3.6/dist-packages (from sparqlwrapper) (4.2.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (2.4.6)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib>=4.0->sparqlwrapper) (1.12.0)\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iIkvlmxJ2f6",
        "colab_type": "code",
        "outputId": "b8886199-f1ba-41a8-9378-cc8fb90f25d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "# Modules d'archivage et traitement de fichiers\n",
        "import io\n",
        "import csv\n",
        "import json\n",
        "import pandas\n",
        "\n",
        "# Module de gestion des date sous python\n",
        "import datetime\n",
        "\n",
        "# Modules de requêtage HTTP et Webscraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Modules d'accès KB's et SPARQL\n",
        "import sys\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Modules de traitements linguistiques, et réseaux neuronals\n",
        "import spacy\n",
        "import fr_core_news_sm\n",
        "\n",
        "# Modules de traitements linguistiques, et réseaux neuronals\n",
        "import keras\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKV30_b0LKyq",
        "colab_type": "text"
      },
      "source": [
        "# Projet Annuel : *Génération de Biograhies Automatique à l'aide de Réseaux de Neurones (LSTM)*\n",
        "\n",
        "Afin de réaliser ce projet, nous nous aiderons de bases de connaissances (*KB's*) pour l'extraction de nos données et par la suite l'entrainement de nôtre RN.\n",
        "\n",
        "Pour cela, nous cinderons en deux grandes étapes la réalisation de ce travail que sont :\n",
        "\n",
        "\n",
        "1.   Extraction et épuration des données des KB's.\n",
        "2.   Entrainement du RN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPjZc4aPLTun",
        "colab_type": "text"
      },
      "source": [
        "# **Première Etape :**  Extraction des données\n",
        "\n",
        "Après une étude de la conception des deux grandes bases de connaissances que sont *DBPédia* et *WikiData*, nous sommes arrivés à la conclusion qu'une utilisation exclusive de l'une des deux bases de connaissances ne serait pas optimale, et donc nous opterons par la suite pour une combinaison des deux, et cela comme suit :\n",
        "\n",
        "\n",
        "1.   L'extraction des propriétés se fera de *WikiData*.\n",
        "2.   L'extraction des présentations/abstract se fera de *DBPédia*.\n",
        "\n",
        "Pour se faire, nous utiliseront le langage d'intérogation de bases de connaissances *SparQL*, et nous implémenterons se dernier via *Python*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEn2_VzHMReI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Comme spécifié plus haut, la première étape de nôtre étude est la construction du dataset 'fr', et pour cela nous aurons besoins\n",
        "    d'extraire des données de grandes bases de connaissances, combiné à cela, un peu de web scraping.\n",
        "  \n",
        "  En suivant une approche méthodologique, nous commencerons tout d'abord par la définition des différents algorithmes utiles à nos\n",
        "    travaux, que sont les suivants:\n",
        "\n",
        "    ○ Intérrogation et extraction des données des différentes KB's.\n",
        "    ○ Création et sauvegarde des fichiers de dataset.\n",
        "    ○ Formatage et combinaisons des différents fichiers de données.\n",
        "\"\"\"\n",
        "\n",
        "# https://rdflib.github.io/sparqlwrapper/\n",
        "\n",
        "def get_results(endpoint_url, query):\n",
        "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
        "    # TODO adjust user agent; see https://w.wiki/CX6\n",
        "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    return sparql.query().convert()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBqOFyIgQTBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_abstracts_file():\n",
        "  # Nous créons ensuite un nouveau dictionnaire, dont la charge est l'association de chaque entitées avec son abstract.\n",
        "  abstracts = {}\n",
        "  endpoint_url = 'http://fr.dbpedia.org/sparql'\n",
        "\n",
        "  # Définition de la requête SPARQL à exécuter. (nous restreignons de lignes remontées dû à la latence serveur) \n",
        "  query = \"\"\"\n",
        "    PREFIX dbo:<http://dbpedia.org/ontology/>\n",
        "    PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "    select ?acteurLabel ?abstract \n",
        "    where {\n",
        "      ?acteur a dbo:Actor .\n",
        "      ?acteur dbo:abstract ?abstract .\n",
        "      ?acteur rdfs:label ?acteurLabel .\n",
        "      FILTER (LANG(?abstract)='fr' and LANG(?acteurLabel)='en')\n",
        "    } LIMIT 20000\"\"\"\n",
        "\n",
        "  results = get_results(endpoint_url, query)\n",
        "\n",
        "  # Ici nous déclarons/créons un fichier csv porterant le nom de \"actors_abstracts\" et contenant une association: nom => abstract.\n",
        "  with open('drive/My Drive/Colab Notebooks/actors_abstracts.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Nous insérons dabord l'entête de nôtre fichier, pour une meilleure lisibilité.\n",
        "    writer.writerow([\n",
        "          'name',\n",
        "          'abstract'\n",
        "    ])\n",
        "    \n",
        "    for result in results[\"results\"][\"bindings\"]:\n",
        "      writer.writerow([\n",
        "          result[\"acteurLabel\"][\"value\"],\n",
        "          result[\"abstract\"][\"value\"]\n",
        "      ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-EgAgay7431",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cette fonction sert au formatage des dates.\n",
        "def mounth_converter(mounth):\n",
        "  if mounth == \"01\":\n",
        "    return \"janvier\"\n",
        "  elif mounth == \"02\":\n",
        "    return \"février\"\n",
        "  elif mounth == \"03\":\n",
        "    return \"mars\"\n",
        "  elif mounth == \"04\":\n",
        "    return \"avril\"\n",
        "  elif mounth == \"05\":\n",
        "    return \"mai\"\n",
        "  elif mounth == \"06\":\n",
        "    return \"juin\"\n",
        "  elif mounth == \"07\":\n",
        "    return \"juillet\"\n",
        "  elif mounth == \"08\":\n",
        "    return \"août\"\n",
        "  elif mounth == \"09\":\n",
        "    return \"septembre\"\n",
        "  elif mounth == \"10\":\n",
        "    return \"octobre\"\n",
        "  elif mounth == \"11\":\n",
        "    return \"novembre\"\n",
        "  else:\n",
        "    return \"décembre\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwXkhmJTsJ2",
        "colab_type": "text"
      },
      "source": [
        "# Pourquoi une utilisation combinée de deux bases de connaissances ?\n",
        "\n",
        "Pour justifier un tel choix, il nous faut tout d'abord introduire les structures de données que proposent *Wikidata* et *DBPédia* :\n",
        "\n",
        "# **WikiData**\n",
        "Cette KB est très populaire du fait de sa liaison directe avec l'encyclopédie universelle et collaborative qu'est Wikipédia, ainsi que sa facilité d'utilisation. WikiData propose 78_979_737 entités, contrairement à Wikipédia qui elle n'en propose que 30 millions d'articles.\n",
        "Malgré ces avantages, WikiData ne propose aucun résumé de ses entités, s'appuyant principalement sur le référencement de ses articles sur Wikipédia, mais comme vous l'aurez sans doute remarqué, pas toute les entités définies sur WikiData sont présentes sur l'encyclopédie Wikipédia, et cela nous amène à nôtre prochain point.\n",
        "\n",
        "# **DBPédia**\n",
        "Cette dernière fut conçue par ses auteurs comme le « noyaux du Web émergent de l'open data ». En 2010, elle décrivait plus 3,4 millions d'entités distinctes. De nos jours, cette dernière compte plus de 3 milliards d'informations (RDF).\n",
        "DBPédia, s'étant principalement basée sur Wikipédia lors de sa construction, celle-ci contient entre autre de petits paragraphes dans différentes langues décrivants l'entité concernée. Mais dû au fait, d'un certains niveau de difficulté rencontrée lors de son exploitation, nous ne pûmes nous reposer exclusivement sur cette dernière.\n",
        "\n",
        "# **Idée derrière la combinaison**\n",
        "Comme précisée plus haut, *DBPédia* se basant plus sur une construction de graphe ontologique qu'un simple filtrage de données, il nous fut presque impossible d'en tirer le maximum. Pour cela, nous avons dû nous adapter à la situation, et procéder de la sorte :\n",
        "\n",
        "\n",
        "1.   Extraction de 20_000 instances de { \"nom\" : \"abstract\" } à partir de *DBPédia*.\n",
        "2.   Boucler sur ces noms, afin d'accéder à la page *Wikipédia*, si cette dernière existe, pour enfin pouvoir utiliser du \"web scraping\" dans le but d'extraire l'id *WikiData* associé à l'entité.\n",
        "3.   Intéroger *WikiData* pour l'extraction des propriétés qui nous intéressent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV7N-tzQcoXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset():\n",
        "  # List contenant les noms d'acteurs retenus.\n",
        "  data_name = {}\n",
        "\n",
        "  # 1: Extraction des noms du fichier csv et les mettre dans une liste.\n",
        "  with open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/actors_abstracts.csv', 'r', encoding=\"utf-8\") as actor_abs:\n",
        "    csvReader = csv.DictReader(actor_abs)\n",
        "    for row in csvReader:\n",
        "      data_name[ row['name'] ] = row['abstract']\n",
        "\n",
        "  print(\"Noms enregistrés avec succès !!!\")\n",
        "\n",
        "  # List contenant les ID's de nos acteurs.\n",
        "  identifiers = []\n",
        "\n",
        "  tmp = 1\n",
        "\n",
        "  lignes = []\n",
        "\n",
        "  # 2: Boucler sur les noms, et extraire la page wikipédia.\n",
        "  for name in data_name:\n",
        "\n",
        "    # Tout URL Wikipédia possède le format suivant :\n",
        "    r = requests.get(\"https://fr.wikipedia.org/wiki/\"+name)\n",
        "\n",
        "    # Extraction de l'article sous son format HTML.\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    try:\n",
        "      # Le lien WikiData étant dans une balise <li> du site, nous allons extraire cette dernière.\n",
        "      balise = soup.find('li', attrs={ 'id':'t-wikibase' })\n",
        "      # L'identifiant WikiData d'une entité est toujours inscrit en fin de lien, à partir de l'emplacement 49.\n",
        "      identifiers.append(balise.find('a')['href'][49:])\n",
        "\n",
        "      print(\"Acteur n° : \",tmp)\n",
        "      \n",
        "    except AttributeError:\n",
        "      # Si l'on ne trouve pas un lien directement, nous supprimant le tuple concerné.\n",
        "      lignes.append(name)\n",
        "      print(\"Acteur n° : \",tmp,\" supprimé...\")\n",
        "\n",
        "    tmp += 1\n",
        "  \n",
        "  for nom in lignes:\n",
        "    data_name.pop(nom)\n",
        "\n",
        "  print(\"Identifiants enregistrés avec succès !!!\")\n",
        "\n",
        "  # Dictionnaire contenant les noms, dates et lieux de naissance.\n",
        "  dico = {}\n",
        "\n",
        "  # Intérroger WikiData pour extraire les informations scalaires de nos acteurs.\n",
        "  for id in identifiers:\n",
        "    # Définition de la requête SPARQL.\n",
        "    query = \"\"\"SELECT ?acteurLabel ?birthdateLabel ?birthplaceLabel\n",
        "      WHERE {\n",
        "        VALUES ?acteur {\n",
        "          wd:\"\"\"+id+\"\"\"\n",
        "        }\n",
        "        \n",
        "        ?acteur wdt:P569 ?birthdateLabel .\n",
        "        ?acteur wdt:P19 ?birthplace .\n",
        "        \n",
        "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
        "      }\"\"\"\n",
        "\n",
        "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "    result = get_results(endpoint_url, query)\n",
        "\n",
        "    print(\"Requête exécutée avec succès !!!\")\n",
        "\n",
        "    # List contenant tous les postes occupés par un acteur.\n",
        "    occup = []\n",
        "    \n",
        "    query = \"\"\"# acteur\n",
        "      SELECT ?acteurLabel ?occupationLabel WHERE {\n",
        "        VALUES ?acteur {\n",
        "          wd:\"\"\"+id+\"\"\"\n",
        "        }\n",
        "        ?acteur wdt:P106 ?occupation .\n",
        "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
        "      }\"\"\"\n",
        "\n",
        "    result_ = get_results(endpoint_url, query)\n",
        "\n",
        "    print(\"Professions enregistrées avec succès !!!\")\n",
        "\n",
        "    for _ in result_[\"results\"][\"bindings\"]:\n",
        "      # En utilisant un tableau associatif, nous réglons aussi le problème des doublons.\n",
        "      occup.append(_['occupationLabel']['value'])\n",
        "\n",
        "    endpoint_url = \"http://dbpedia.org/snorql/\"\n",
        "\n",
        "    query = \"\"\"\"\n",
        "      PREFIX dbo:<http://dbpedia.org/ontology/>\n",
        "        SELECT ?titre WHERE {\n",
        "        ?acteur foaf:name \"\"\"+result[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"]+\"\"\"@en.\n",
        "        ?film dbo:starring ?acteur.\n",
        "        ?film foaf:name ?titre.\n",
        "        filter(LANG(?titre)=\"en\")\n",
        "      }\"\"\"\n",
        "\n",
        "    m_result = get_results(endpoint_url, query)\n",
        "\n",
        "    movies = []\n",
        "    \n",
        "    for _ in m_result[\"results\"][\"bindings\"]:\n",
        "      # On utilisant un tableau associatif, nous réglons aussi le problème des doublons.\n",
        "      movies.append(_['titre']['value'])\n",
        "\n",
        "    print(\"Films enregistrés avec succès !!!\")\n",
        "\n",
        "    try:\n",
        "\n",
        "      date = str(datetime.datetime.strptime(result[\"results\"][\"bindings\"][0][\"birthdateLabel\"][\"value\"], \"%Y-%m-%dT%H:%M:%SZ\"))[:10].split('-')\n",
        "      date[1] = mounth_converter(date[1])\n",
        "\n",
        "      date = date[2]+' '+date[1]+' '+date[0]\n",
        "\n",
        "      dico[id] = [\n",
        "              result[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"],\n",
        "              date,\n",
        "              result[\"results\"][\"bindings\"][0][\"birthplaceLabel\"][\"value\"],\n",
        "              occup,\n",
        "              movies,\n",
        "              data_name[result[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"]]\n",
        "      ]\n",
        "\n",
        "    except ValueError:\n",
        "      # En cas d'inconformité de la donnée, cette dernière ne sera aps sauvegardée.\n",
        "      continue\n",
        "\n",
        "  print(\"Sauvegarde sans du dataset en cours...\")\n",
        "\n",
        "  # Enfin, nous combinons nos fichiers, pour construire un dataset complet sous format json.\n",
        "  with open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/dataset.txt', 'w') as outfile:\n",
        "    json.dump(occupations_by_id, outfile, ensure_ascii=False)\n",
        "\n",
        "  print(\"Sauvegarde effectuée avec succès !!!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0wSc28IHY6",
        "colab_type": "code",
        "outputId": "88cfd4ce-fe5c-4756-af90-0fa080089a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "create_dataset()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noms enregistrés avec succès !!!\n",
            "Acteur n° :  1\n",
            "Acteur n° :  2\n",
            "Acteur n° :  3\n",
            "Acteur n° :  4\n",
            "Acteur n° :  5\n",
            "Acteur n° :  6\n",
            "Acteur n° :  7\n",
            "Acteur n° :  8\n",
            "Acteur n° :  9\n",
            "Acteur n° :  10\n",
            "Acteur n° :  11\n",
            "Acteur n° :  12\n",
            "Acteur n° :  13\n",
            "Acteur n° :  14\n",
            "Acteur n° :  15\n",
            "Acteur n° :  16\n",
            "Acteur n° :  17\n",
            "Acteur n° :  18\n",
            "Acteur n° :  19\n",
            "Acteur n° :  20\n",
            "Acteur n° :  21\n",
            "Acteur n° :  22\n",
            "Acteur n° :  23\n",
            "Acteur n° :  24\n",
            "Acteur n° :  25\n",
            "Acteur n° :  26\n",
            "Acteur n° :  27\n",
            "Acteur n° :  28\n",
            "Acteur n° :  29\n",
            "Acteur n° :  30\n",
            "Acteur n° :  31  supprimé...\n",
            "Acteur n° :  32\n",
            "Acteur n° :  33\n",
            "Acteur n° :  34\n",
            "Acteur n° :  35\n",
            "Acteur n° :  36\n",
            "Acteur n° :  37  supprimé...\n",
            "Acteur n° :  38  supprimé...\n",
            "Acteur n° :  39\n",
            "Acteur n° :  40\n",
            "Acteur n° :  41\n",
            "Acteur n° :  42\n",
            "Acteur n° :  43\n",
            "Acteur n° :  44\n",
            "Acteur n° :  45\n",
            "Acteur n° :  46\n",
            "Acteur n° :  47\n",
            "Acteur n° :  48\n",
            "Acteur n° :  49\n",
            "Acteur n° :  50  supprimé...\n",
            "Acteur n° :  51\n",
            "Acteur n° :  52\n",
            "Acteur n° :  53\n",
            "Acteur n° :  54\n",
            "Acteur n° :  55  supprimé...\n",
            "Acteur n° :  56\n",
            "Acteur n° :  57\n",
            "Acteur n° :  58\n",
            "Acteur n° :  59\n",
            "Acteur n° :  60\n",
            "Acteur n° :  61\n",
            "Acteur n° :  62\n",
            "Acteur n° :  63\n",
            "Acteur n° :  64\n",
            "Acteur n° :  65\n",
            "Acteur n° :  66\n",
            "Acteur n° :  67\n",
            "Acteur n° :  68\n",
            "Acteur n° :  69\n",
            "Acteur n° :  70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dc4c5c72201c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'create_dataset()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-856b7876b95b>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Tout URL Wikipédia possède le format suivant :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://fr.wikipedia.org/wiki/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Extraction de l'article sous son format HTML.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             ssl_context=context)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[1;32m    343\u001b[0m             or IS_SECURETRANSPORT):\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    405\u001b[0m                          \u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                          _context=self, _session=session)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     def wrap_bio(self, incoming, outgoing, server_side=False,\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\u001b[0m\n\u001b[1;32m    815\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOuANTUWQsrP",
        "colab_type": "text"
      },
      "source": [
        "# **Deuxième Etape :** Epuration des données (*data cleaning*)\n",
        "Après extraction des données, nous nous intéressons désormais au nettoyage de ces dernières, et cela via 2 procédés que sont :\n",
        "\n",
        "\n",
        "1.   Correspondance des données et leurs labels dans les abstracts. (*matching*)\n",
        "2.   Extraction d'un paterne de succession récurent entre les différentes propriétées. (*graph*)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXRmQ7cZRGOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "  dataset_json = open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/dataset.txt', 'r', encoding='utf-8')\n",
        "  dataset_dict = json.load(dataset_json)\n",
        "  dataset_json.close()\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2zP53ZvQ4Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corresponding_data():\n",
        "\n",
        "  # Dictionnaire contenant nôtre dataset.\n",
        "  dataset = load_dataset()\n",
        "\n",
        "  # Pour chaque acteur.\n",
        "  for _ in dataset:\n",
        "\n",
        "    # Un attribut nous permettant de nous situer dans l'enregistrement.\n",
        "    property = 0\n",
        "\n",
        "    # On boucle sur ses propriétés.\n",
        "    for propriete in dataset.get(_) :\n",
        "      # Si ce sont des films ou des professions, on devra les parcourir aussi.\n",
        "      if type(propriete) == list:\n",
        "        for sous_propriete in propriete:\n",
        "          if property == 3 : # On est aux professions.\n",
        "            dataset[_][5] = dataset[_][5].replace(sous_propriete, \"<PROPERTY_OCCUPATIONS>\")\n",
        "          elif property == 4 : # On est aux films.\n",
        "            dataset[_][5] = dataset[_][5].replace(sous_propriete, \"<PROPERTY_MOVIES>\")\n",
        "      else:\n",
        "        if property == 0 : # Nom.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_NAME>\")\n",
        "        elif property == 1 : # Date de naissance.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_BIRTHDATE>\")\n",
        "        elif property == 2 : # Lieu de naissance.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_BIRTHPLACE>\")\n",
        "        \n",
        "      # Pour se situer dans le dictionnaire.\n",
        "      property += 1\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtjwkSM2yo78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = corresponding_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6kfntu9Ww7",
        "colab_type": "text"
      },
      "source": [
        "# **Troisième Etape :** Conception du modèle des réseaux de neurones & Entrainement\n",
        "\n",
        "Dans ce qui va suivre, une partie conceptuelle ainsi qu'applicative \"recyclée\" dans nôtre projet, faute de temps, nous ne pûmes totalement la réétudier ni l'adapter au mieux à nôtre projet. Pour de plus amples informations veuillez vous référer à Belkacemi Ryad & Kesouri Manil.\n",
        "\n",
        "L'algorithme qui suit consiste à lire nôs instances, et essayer de générer un semblent de texte conforme au format d'une biographie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzfgc6Vxy2mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Accès au fichier à refaire\n",
        "with open('drive/My Drive/Colab Notebooks/dataset/abstract_dataset.txt', encoding='utf-8') as f:\n",
        "  text = f.read().lower()\n",
        "\n",
        "chars = sorted (list(set(text)))\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text)-40, 3):\n",
        "  sentences.append(text[i: i+40])\n",
        "  next_chars.append(text[i+40])\n",
        "\n",
        "x = np.zeros((len(sentences), 40, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  \n",
        "  y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.LSTM(128, input_shape=(40, len(chars))))\n",
        "model.add(keras.layers.Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer= keras.optimizers.RMSprop(lr=0.01))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74tWGTKP8zje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds)/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  \n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXtkbkb81bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x, y, batch_size=18, epochs=60)\n",
        "\n",
        "start_index = random.randint(0, len(text)-40-1)\n",
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "  generated = ''\n",
        "  sentence = text[start_index: start_index+40]\n",
        "  generated += sentence\n",
        "\n",
        "  for i in range(600):\n",
        "    x_pred = np.zeros((1, 40, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      x_pred[0, t, char_indices[char]] = 1\n",
        "\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds, diversity)\n",
        "    next_char = indices_char[next_index]\n",
        "\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:] + next_char\n",
        "\n",
        "with open('drive/My Drive/Colab Notebooks/biographie.txt', 'w') as f:\n",
        "  f.write(generated)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}