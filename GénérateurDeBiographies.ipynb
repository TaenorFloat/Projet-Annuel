{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GénérateurDeBiographies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1U7BgcRyuU2lkbPJaQwX58UJY9NqnFm9w",
      "authorship_tag": "ABX9TyMDCydJ4wjD903Gc85LmuHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaenorFloat/Projet-Annuel/blob/master/G%C3%A9n%C3%A9rateurDeBiographies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PASvxrryJUAh",
        "colab_type": "code",
        "outputId": "913a3a2d-1b7d-42e1-f059-e064df54b01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UFBZTgYJvci",
        "colab_type": "code",
        "outputId": "858e751f-bc66-4291-ee9f-75070e5298bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "! pip install sparqlwrapper\n",
        "! python -m spacy download fr\n",
        "! python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sparqlwrapper in /usr/local/lib/python3.6/dist-packages (1.8.5)\n",
            "Requirement already satisfied: rdflib>=4.0 in /usr/local/lib/python3.6/dist-packages (from sparqlwrapper) (4.2.2)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (0.6.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib>=4.0->sparqlwrapper) (1.12.0)\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iIkvlmxJ2f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modules d'archivage et traitement de fichiers\n",
        "import io\n",
        "import csv\n",
        "import json\n",
        "import pandas\n",
        "\n",
        "# Module de gestion des date sous python\n",
        "import datetime\n",
        "\n",
        "# Modules de requêtage HTTP et Webscraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Modules d'accès KB's et SPARQL\n",
        "import sys\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Modules de traitements linguistiques, et réseaux neuronals\n",
        "import spacy\n",
        "import fr_core_news_sm\n",
        "\n",
        "# Modules de traitements linguistiques, et réseaux neuronals\n",
        "import keras\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKV30_b0LKyq",
        "colab_type": "text"
      },
      "source": [
        "# Projet Annuel : *Génération de Biograhies Automatique à l'aide de Réseaux de Neurones (LSTM)*\n",
        "\n",
        "Afin de réaliser ce projet, nous nous aiderons de bases de connaissances (*KB's*) pour l'extraction de nos données et par la suite l'entrainement de nôtre RN.\n",
        "\n",
        "Pour cela, nous cinderons en deux grandes étapes la réalisation de ce travail que sont :\n",
        "\n",
        "\n",
        "1.   Extraction et épuration des données des KB's.\n",
        "2.   Entrainement du RN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPjZc4aPLTun",
        "colab_type": "text"
      },
      "source": [
        "# **Première Etape :**  Extraction des données\n",
        "\n",
        "Après une étude de la conception des deux grandes bases de connaissances que sont *DBPédia* et *WikiData*, nous sommes arrivés à la conclusion qu'une utilisation exclusive de l'une des deux bases de connaissances ne serait pas optimale, et donc nous opterons par la suite pour une combinaison des deux, et cela comme suit :\n",
        "\n",
        "\n",
        "1.   L'extraction des propriétés se fera de *WikiData*.\n",
        "2.   L'extraction des présentations/abstract se fera de *DBPédia*.\n",
        "\n",
        "Pour se faire, nous utiliseront le langage d'intérogation de bases de connaissances *SparQL*, et nous implémenterons se dernier via *Python*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEn2_VzHMReI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Comme spécifié plus haut, la première étape de nôtre étude est la construction du dataset 'fr', et pour cela nous aurons besoins\n",
        "    d'extraire des données de grandes bases de connaissances, combiné à cela, un peu de web scraping.\n",
        "  \n",
        "  En suivant une approche méthodologique, nous commencerons tout d'abord par la définition des différents algorithmes utiles à nos\n",
        "    travaux, que sont les suivants:\n",
        "\n",
        "    ○ Intérrogation et extraction des données des différentes KB's.\n",
        "    ○ Création et sauvegarde des fichiers de dataset.\n",
        "    ○ Formatage et combinaisons des différents fichiers de données.\n",
        "\"\"\"\n",
        "\n",
        "# https://rdflib.github.io/sparqlwrapper/\n",
        "\n",
        "def get_results(endpoint_url, query):\n",
        "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
        "    # TODO adjust user agent; see https://w.wiki/CX6\n",
        "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    return sparql.query().convert()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBqOFyIgQTBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_abstracts_file():\n",
        "  # Nous créons ensuite un nouveau dictionnaire, dont la charge est l'association de chaque entitées avec son abstract.\n",
        "  abstracts = {}\n",
        "  endpoint_url = 'http://fr.dbpedia.org/sparql'\n",
        "\n",
        "  # Définition de la requête SPARQL à exécuter. (nous restreignons de lignes remontées dû à la latence serveur) \n",
        "  query = \"\"\"\n",
        "    PREFIX dbo:<http://dbpedia.org/ontology/>\n",
        "    PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "    select ?acteurLabel ?abstract \n",
        "    where {\n",
        "      ?acteur a dbo:Actor .\n",
        "      ?acteur dbo:abstract ?abstract .\n",
        "      ?acteur rdfs:label ?acteurLabel .\n",
        "      FILTER (LANG(?abstract)='fr' and LANG(?acteurLabel)='en')\n",
        "    } LIMIT 20000\"\"\"\n",
        "\n",
        "  results = get_results(endpoint_url, query)\n",
        "\n",
        "  # Ici nous déclarons/créons un fichier csv porterant le nom de \"actors_abstracts\" et contenant une association: nom => abstract.\n",
        "  with open('drive/My Drive/Colab Notebooks/actors_abstracts.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Nous insérons dabord l'entête de nôtre fichier, pour une meilleure lisibilité.\n",
        "    writer.writerow([\n",
        "          'name',\n",
        "          'abstract'\n",
        "    ])\n",
        "    \n",
        "    for result in results[\"results\"][\"bindings\"]:\n",
        "      writer.writerow([\n",
        "          result[\"acteurLabel\"][\"value\"],\n",
        "          result[\"abstract\"][\"value\"]\n",
        "      ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-EgAgay7431",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cette fonction sert au formatage des dates.\n",
        "def mounth_converter(mounth):\n",
        "  if mounth == \"01\":\n",
        "    return \"janvier\"\n",
        "  elif mounth == \"02\":\n",
        "    return \"février\"\n",
        "  elif mounth == \"03\":\n",
        "    return \"mars\"\n",
        "  elif mounth == \"04\":\n",
        "    return \"avril\"\n",
        "  elif mounth == \"05\":\n",
        "    return \"mai\"\n",
        "  elif mounth == \"06\":\n",
        "    return \"juin\"\n",
        "  elif mounth == \"07\":\n",
        "    return \"juillet\"\n",
        "  elif mounth == \"08\":\n",
        "    return \"août\"\n",
        "  elif mounth == \"09\":\n",
        "    return \"septembre\"\n",
        "  elif mounth == \"10\":\n",
        "    return \"octobre\"\n",
        "  elif mounth == \"11\":\n",
        "    return \"novembre\"\n",
        "  else:\n",
        "    return \"décembre\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwXkhmJTsJ2",
        "colab_type": "text"
      },
      "source": [
        "# Pourquoi une utilisation combinée de deux bases de connaissances ?\n",
        "\n",
        "Pour justifier un tel choix, il nous faut tout d'abord introduire les structures de données que proposent *Wikidata* et *DBPédia* :\n",
        "\n",
        "# **WikiData**\n",
        "Cette KB est très populaire du fait de sa liaison directe avec l'encyclopédie universelle et collaborative qu'est Wikipédia, ainsi que sa facilité d'utilisation. WikiData propose 78_979_737 entités, contrairement à Wikipédia qui elle n'en propose que 30 millions d'articles.\n",
        "Malgré ces avantages, WikiData ne propose aucun résumé de ses entités, s'appuyant principalement sur le référencement de ses articles sur Wikipédia, mais comme vous l'aurez sans doute remarqué, pas toute les entités définies sur WikiData sont présentes sur l'encyclopédie Wikipédia, et cela nous amène à nôtre prochain point.\n",
        "\n",
        "# **DBPédia**\n",
        "Cette dernière fut conçue par ses auteurs comme le « noyaux du Web émergent de l'open data ». En 2010, elle décrivait plus 3,4 millions d'entités distinctes. De nos jours, cette dernière compte plus de 3 milliards d'informations (RDF).\n",
        "DBPédia, s'étant principalement basée sur Wikipédia lors de sa construction, celle-ci contient entre autre de petits paragraphes dans différentes langues décrivants l'entité concernée. Mais dû au fait, d'un certains niveau de difficulté rencontrée lors de son exploitation, nous ne pûmes nous reposer exclusivement sur cette dernière.\n",
        "\n",
        "# **Idée derrière la combinaison**\n",
        "Comme précisée plus haut, *DBPédia* se basant plus sur une construction de graphe ontologique qu'un simple filtrage de données, il nous fut presque impossible d'en tirer le maximum. Pour cela, nous avons dû nous adapter à la situation, et procéder de la sorte :\n",
        "\n",
        "\n",
        "1.   Extraction de 20_000 instances de { \"nom\" : \"abstract\" } à partir de *DBPédia*.\n",
        "2.   Boucler sur ces noms, afin d'accéder à la page *Wikipédia*, si cette dernière existe, pour enfin pouvoir utiliser du \"web scraping\" dans le but d'extraire l'id *WikiData* associé à l'entité.\n",
        "3.   Intéroger *WikiData* pour l'extraction des propriétés qui nous intéressent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV7N-tzQcoXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset():\n",
        "  # List contenant les noms d'acteurs retenus.\n",
        "  data_name = {}\n",
        "\n",
        "  # 1: Extraction des noms du fichier csv et les mettre dans une liste.\n",
        "  with open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/actors_abstracts.csv', 'r', encoding=\"utf-8\") as actor_abs:\n",
        "    csvReader = csv.DictReader(actor_abs)\n",
        "    for row in csvReader:\n",
        "      data_name[ row['name'] ] = row['abstract']\n",
        "\n",
        "  print(\"Noms enregistrés avec succès !!!\")\n",
        "\n",
        "  # List contenant les ID's de nos acteurs.\n",
        "  identifiers = []\n",
        "\n",
        "  tmp = 1\n",
        "\n",
        "  lignes = []\n",
        "\n",
        "  # 2: Boucler sur les noms, et extraire la page wikipédia.\n",
        "  for name in data_name:\n",
        "\n",
        "    # Tout URL Wikipédia possède le format suivant :\n",
        "    r = requests.get(\"https://fr.wikipedia.org/wiki/\"+name)\n",
        "\n",
        "    # Extraction de l'article sous son format HTML.\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    try:\n",
        "      # Le lien WikiData étant dans une balise <li> du site, nous allons extraire cette dernière.\n",
        "      balise = soup.find('li', attrs={ 'id':'t-wikibase' })\n",
        "      # L'identifiant WikiData d'une entité est toujours inscrit en fin de lien, à partir de l'emplacement 49.\n",
        "      identifiers.append(balise.find('a')['href'][49:])\n",
        "\n",
        "      print(\"Acteur n° : \",tmp)\n",
        "      \n",
        "    except AttributeError:\n",
        "      # Si l'on ne trouve pas un lien directement, nous supprimant le tuple concerné.\n",
        "      lignes.append(name)\n",
        "      print(\"Acteur n° : \",tmp,\" supprimé...\")\n",
        "\n",
        "    tmp += 1\n",
        "  \n",
        "  for nom in lignes:\n",
        "    data_name.pop(nom)\n",
        "\n",
        "  print(\"Identifiants enregistrés avec succès !!!\")\n",
        "\n",
        "  # Dictionnaire contenant les noms, dates et lieux de naissance.\n",
        "  dico = {}\n",
        "\n",
        "  # Intérroger WikiData pour extraire les informations scalaires de nos acteurs.\n",
        "  for id in identifiers:\n",
        "    # Définition de la requête SPARQL.\n",
        "    query = \"\"\"SELECT ?acteurLabel ?birthdateLabel ?birthplaceLabel\n",
        "      WHERE {\n",
        "        VALUES ?acteur {\n",
        "          wd:\"\"\"+id+\"\"\"\n",
        "        }\n",
        "        \n",
        "        ?acteur wdt:P569 ?birthdateLabel .\n",
        "        ?acteur wdt:P19 ?birthplace .\n",
        "        \n",
        "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
        "      }\"\"\"\n",
        "\n",
        "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "    result = get_results(endpoint_url, query)\n",
        "\n",
        "    print(\"Requête exécutée avec succès !!!\")\n",
        "\n",
        "    # List contenant tous les postes occupés par un acteur.\n",
        "    occup = []\n",
        "    \n",
        "    query = \"\"\"# acteur\n",
        "      SELECT ?acteurLabel ?occupationLabel WHERE {\n",
        "        VALUES ?acteur {\n",
        "          wd:\"\"\"+id+\"\"\"\n",
        "        }\n",
        "        ?acteur wdt:P106 ?occupation .\n",
        "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
        "      }\"\"\"\n",
        "\n",
        "    result_ = get_results(endpoint_url, query)\n",
        "\n",
        "    print(\"Professions enregistrées avec succès !!!\")\n",
        "\n",
        "    for _ in result_[\"results\"][\"bindings\"]:\n",
        "      # En utilisant un tableau associatif, nous réglons aussi le problème des doublons.\n",
        "      occup.append(_['occupationLabel']['value'])\n",
        "\n",
        "    endpoint_url = \"http://dbpedia.org/sparql/\"\n",
        "\n",
        "    query = '''\n",
        "      PREFIX dbo:<http://dbpedia.org/ontology/>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "        SELECT ?titre WHERE {\n",
        "        ?acteur foaf:name \"'''+result_[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"]+'''\"@en.\n",
        "        ?film dbo:starring ?acteur.\n",
        "        ?film foaf:name ?titre.\n",
        "        filter(LANG(?titre)=\"en\")\n",
        "      }'''\n",
        "\n",
        "    m_result = get_results(endpoint_url, query)\n",
        "\n",
        "    movies = []\n",
        "    \n",
        "    for _ in m_result[\"results\"][\"bindings\"]:\n",
        "      # On utilisant un tableau associatif, nous réglons aussi le problème des doublons.\n",
        "      movies.append(_['titre']['value'])\n",
        "\n",
        "    try:\n",
        "\n",
        "      date = str(datetime.datetime.strptime(result[\"results\"][\"bindings\"][0][\"birthdateLabel\"][\"value\"], \"%Y-%m-%dT%H:%M:%SZ\"))[:10].split('-')\n",
        "      date[1] = mounth_converter(date[1])\n",
        "\n",
        "      date = date[2]+' '+date[1]+' '+date[0]\n",
        "\n",
        "      dico[id] = [\n",
        "              result[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"],\n",
        "              date,\n",
        "              result[\"results\"][\"bindings\"][0][\"birthplaceLabel\"][\"value\"],\n",
        "              occup,\n",
        "              movies,\n",
        "              data_name[result[\"results\"][\"bindings\"][0][\"acteurLabel\"][\"value\"]]\n",
        "      ]\n",
        "\n",
        "    except ValueError:\n",
        "      # En cas d'inconformité de la donnée, cette dernière ne sera aps sauvegardée.\n",
        "      continue\n",
        "\n",
        "  print(\"Sauvegarde sans du dataset en cours...\")\n",
        "\n",
        "  # Enfin, nous combinons nos fichiers, pour construire un dataset complet sous format json.\n",
        "  with open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/dataset.txt', 'w') as outfile:\n",
        "    json.dump(dico, outfile, ensure_ascii=False)\n",
        "\n",
        "  print(\"Sauvegarde effectuée avec succès !!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0wSc28IHY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "create_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOuANTUWQsrP",
        "colab_type": "text"
      },
      "source": [
        "# **Deuxième Etape :** Epuration des données (*data cleaning*)\n",
        "Après extraction des données, nous nous intéressons désormais au nettoyage de ces dernières, et cela via 2 procédés que sont :\n",
        "\n",
        "\n",
        "1.   Correspondance des données et leurs labels dans les abstracts. (*matching*)\n",
        "2.   Extraction d'un paterne de succession récurent entre les différentes propriétées. (*graph*)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXRmQ7cZRGOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "  dataset_json = open('drive/My Drive/Colab Notebooks/Projet Annuel/dataset/dataset.txt', 'r', encoding='utf-8')\n",
        "  dataset_dict = json.load(dataset_json)\n",
        "  dataset_json.close()\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2zP53ZvQ4Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corresponding_data():\n",
        "\n",
        "  # Dictionnaire contenant nôtre dataset.\n",
        "  dataset = load_dataset()\n",
        "\n",
        "  # Pour chaque acteur.\n",
        "  for _ in dataset:\n",
        "\n",
        "    # Un attribut nous permettant de nous situer dans l'enregistrement.\n",
        "    property = 0\n",
        "\n",
        "    # On boucle sur ses propriétés.\n",
        "    for propriete in dataset.get(_) :\n",
        "      # Si ce sont des films ou des professions, on devra les parcourir aussi.\n",
        "      if type(propriete) == list:\n",
        "        for sous_propriete in propriete:\n",
        "          if property == 3 : # On est aux professions.\n",
        "            dataset[_][5] = dataset[_][5].replace(sous_propriete, \"<PROPERTY_OCCUPATIONS>\")\n",
        "          elif property == 4 : # On est aux films.\n",
        "            dataset[_][5] = dataset[_][5].replace(sous_propriete, \"<PROPERTY_MOVIES>\")\n",
        "      else:\n",
        "        if property == 0 : # Nom.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_NAME>\")\n",
        "        elif property == 1 : # Date de naissance.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_BIRTHDATE>\")\n",
        "        elif property == 2 : # Lieu de naissance.\n",
        "          dataset[_][5] = dataset[_][5].replace(propriete, \"<PROPERTY_BIRTHPLACE>\")\n",
        "        \n",
        "      # Pour se situer dans le dictionnaire.\n",
        "      property += 1\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtjwkSM2yo78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = corresponding_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6kfntu9Ww7",
        "colab_type": "text"
      },
      "source": [
        "# **Troisième Etape :** Conception du modèle des réseaux de neurones & Entrainement\n",
        "\n",
        "Dans ce qui va suivre, une partie conceptuelle ainsi qu'applicative \"recyclée\" dans nôtre projet, faute de temps, nous ne pûmes totalement la réétudier ni l'adapter au mieux à nôtre projet. Pour de plus amples informations veuillez vous référer à Belkacemi Ryad & Kesouri Manil.\n",
        "\n",
        "L'algorithme qui suit consiste à lire nôs instances, et essayer de générer un semblent de texte conforme au format d'une biographie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQmft5NMYBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement des abstracts.\n",
        "mon_dict = load_dataset()\n",
        "\n",
        "# Construction d'un set d'abstracts.\n",
        "abstracts = []\n",
        "for _ in mon_dict:\n",
        "  abstracts.append(mon_dict[_][5])\n",
        "\n",
        "# Construction du vocabulaire.\n",
        "vocabulaire = []\n",
        "for abstract in abstracts:\n",
        "  for mot in abstract.split(\" \"):\n",
        "    for s_mot in mot.split(\"'\"):\n",
        "      vocabulaire.append(s_mot)\n",
        "\n",
        "# On faisant le cast suivant, on se débarasse des redondence, et l'on obtient l'ensemble des mots possibles apparaissant dans tout les abstracts.\n",
        "vocabulaire = sorted(list(set(vocabulaire)))\n",
        "\n",
        "# On construit un dictionnaire de codage, comportant le mot clé et son code en tant que valeur.\n",
        "mot_code = dict((c, i) for i, c in enumerate(vocabulaire)) # codage\n",
        "\n",
        "code_mot = dict((i, c) for i, c in enumerate(vocabulaire)) # décodage\n",
        "\n",
        "# Vu que chaque abstract est différent des autres, ces derniers sont aussi différents de par leur taille, et donc nous prendrons le maximum des taille en tant que majorant.\n",
        "max_size = 0\n",
        "\n",
        "for abstract in abstracts:\n",
        "  words = []\n",
        "  for mot in abstract.split(\" \"):\n",
        "    for s_mot in mot.split(\"'\"):\n",
        "      words.append(s_mot)\n",
        "\n",
        "  max_size = max(max_size, len(words))\n",
        "\n",
        "# Cette première matrice de dimension 3 X[i, j, k] où : i est l'ordre de l'abstract, j l'ordre d'un mot dans un abstract d'ordre i et k l'ensemble des mots de code du vocabualire,\n",
        "#   servira à indiquer l'apparition d'un mot dans un text à une position donnée, c'est-à-dire, X[i, j, k] := 1. (True)\n",
        "x = np.zeros((len(abstracts), max_size, len(vocabulaire)), dtype=np.bool)\n",
        "\n",
        "# Cette seconde matrice sert à prodiguer l'information sur le mot précédant le mot actuel selon une position donnée, c'est-à-dire, Y[i, k, j] := codage(mot_pred).\n",
        "y = np.zeros((len(abstracts), len(vocabulaire), max_size), dtype=np.bool)\n",
        "\n",
        "\n",
        "for i, abstract in enumerate(abstracts):\n",
        "  pred = -1 # La valeur -1 indiquant l'abscence de prédécesseur.\n",
        "  words = []\n",
        "  for mot in abstract.split(\" \"):\n",
        "    for s_mots in mot.split(\"'\"):\n",
        "      words.append(s_mot)\n",
        "\n",
        "  for t, mot in enumerate(words):\n",
        "    x[i, t, mot_code[mot]] = 1\n",
        "    y[i, mot_code[mot], t] = pred\n",
        "    pred = mot_code[mot]\n",
        "\n",
        "\n",
        "# Initilisation du modèle d'apprentissage.\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.LSTM(128, input_shape=(max_size, len(vocabulaire))))\n",
        "model.add(keras.layers.Dense(len(vocabulaire), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer= keras.optimizers.RMSprop(lr=0.01))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74tWGTKP8zje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  \n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4v-eFbkDjAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_frequent(List): \n",
        "  counter = 0\n",
        "  num = List[0] \n",
        "    \n",
        "  for i in List: \n",
        "      curr_frequency = List.count(i) \n",
        "      if(curr_frequency> counter): \n",
        "          counter = curr_frequency \n",
        "          num = i \n",
        "\n",
        "  return num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cctkF6PDrTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Si l'on souhaite générer un text qui soit le plus cohérent possible, il nous faut miser sur la fréquence de sa taille parmi tout les autres abstract.\n",
        "medium_size = []\n",
        "\n",
        "for abstract in abstracts:\n",
        "  medium_size.append(len(abstract))\n",
        "\n",
        "medium_size = most_frequent(medium_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXtkbkb81bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x, y, batch_size=18, epochs=60) # Entraînement du modèle.\n",
        "\n",
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "  generated = ''\n",
        "  phrase += '<PROPERTY_NAME>' # Dans la majorité des cas, un abstract commencera par la propriété: nom.\n",
        "  generated += phrase\n",
        "\n",
        "  for i in range(medium_size):\n",
        "    x_pred = np.zeros((1, max_size, len(vocabulaire)))\n",
        "    for t, mot in enumerate(phrase):\n",
        "      x_pred[0, t, char_indices[mot]] = 1 # Indiquer que l'abstract à générer commencera par la propriété: nom.\n",
        "\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds, diversity) # On détermine le prochain mot.\n",
        "    next_word = code_mot[next_index]\n",
        "\n",
        "    generated += next_word\n",
        "    phrase = phrase[1:] + next_word\n",
        "\n",
        "# Sauvegarde dans un fichier.\n",
        "with open('drive/My Drive/Colab Notebooks/Projet Annuel/biographie.txt', 'w') as f:\n",
        "  f.write(generated)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}